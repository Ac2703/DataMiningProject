import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif

try:
    from xgboost import XGBClassifier, XGBRegressor
    HAS_XGBOOST = True
except (ImportError, Exception):
    HAS_XGBOOST = False

CLEANED_DATA_PATH = Path("cleaned_dataset.csv")
RAW_DATA_PATH = Path("large_art_ecommerce_dataset.csv")
PRICE_COLUMN = "Price ($)"
PRICE_CATEGORY_COLUMN = "Price Category"


def ensure_clean_data():
    if CLEANED_DATA_PATH.exists():
        print("Found existing cleaned dataset.")
        return pd.read_csv(CLEANED_DATA_PATH)

    print("Cleaned dataset not found. Running preprocess_data.py ...")
    from preprocess_data import clean_dataset

    clean_dataset()
    return pd.read_csv(CLEANED_DATA_PATH)


def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    
    if "Size" in df.columns:
        df["Size"] = df["Size"].str.replace('"', '')
        size_parts = df["Size"].str.split('x', expand=True)
        df["Width"] = pd.to_numeric(size_parts[0], errors='coerce')
        df["Height"] = pd.to_numeric(size_parts[1], errors='coerce')
        
        df["Area"] = df["Width"] * df["Height"]
        df["AspectRatio"] = df["Width"] / df["Height"]
        df["Perimeter"] = 2 * (df["Width"] + df["Height"])
        df["LogArea"] = np.log1p(df["Area"])
        
        df = df.drop(columns=["Size"])
        
        df["Width"] = df["Width"].fillna(df["Width"].median())
        df["Height"] = df["Height"].fillna(df["Height"].median())
        df["Area"] = df["Area"].fillna(df["Area"].median())
        df["AspectRatio"] = df["AspectRatio"].fillna(df["AspectRatio"].median())
        df["Perimeter"] = df["Perimeter"].fillna(df["Perimeter"].median())
    
    if "Frame" in df.columns:
        df["HasFrame"] = (df["Frame"] == "Yes").astype(int)
    
    if "Copy or Original" in df.columns:
        df["IsOriginal"] = (df["Copy or Original"] == "Original").astype(int)
    
    if "Print or Real" in df.columns:
        df["IsReal"] = (df["Print or Real"] == "Real").astype(int)
    
    if "Area" in df.columns and "Delivery (days)" in df.columns:
        df["Area_Delivery"] = df["Area"] * df["Delivery (days)"]
    
    if "IsOriginal" in df.columns and "IsReal" in df.columns:
        df["Original_Real"] = df["IsOriginal"] * df["IsReal"]
    
    if "HasFrame" in df.columns and "Area" in df.columns:
        df["Frame_Area"] = df["HasFrame"] * df["Area"]
    
    if PRICE_COLUMN in df.columns and "Area" in df.columns:
        df["PricePerArea"] = df[PRICE_COLUMN] / (df["Area"] + 1)
    
    return df


def assign_price_categories(df: pd.DataFrame, method='tertiles') -> pd.DataFrame:
    if PRICE_COLUMN not in df.columns:
        raise ValueError(f"{PRICE_COLUMN} column is required to create price categories.")

    price_series = df[PRICE_COLUMN].astype(float)
    
    if method == 'tertiles':
        quantiles = price_series.quantile([0, 1 / 3, 2 / 3, 1]).values
        labels = ["Low", "Medium", "High"]
    elif method == 'quartiles':
        quantiles = price_series.quantile([0, 0.25, 0.5, 0.75, 1]).values
        labels = ["Very Low", "Low", "Medium", "High"]
    elif method == 'fixed':
        quantiles = [price_series.min() - 1, 400, 650, price_series.max() + 1]
        labels = ["Low", "Medium", "High"]
    else:
        quantiles = price_series.quantile([0, 1 / 3, 2 / 3, 1]).values
        labels = ["Low", "Medium", "High"]
    
    quantiles[0] = price_series.min() - 1
    quantiles[-1] = price_series.max() + 1

    df[PRICE_CATEGORY_COLUMN] = pd.cut(price_series, bins=quantiles, labels=labels, include_lowest=True)
    print(f"\nPrice category distribution ({method}):")
    print(df[PRICE_CATEGORY_COLUMN].value_counts().sort_index())
    return df


def assign_binary_categories(df: pd.DataFrame, threshold='median') -> pd.DataFrame:
    if PRICE_COLUMN not in df.columns:
        raise ValueError(f"{PRICE_COLUMN} column is required.")
    
    price_series = df[PRICE_COLUMN].astype(float)
    
    if threshold == 'median':
        threshold_price = price_series.median()
    elif threshold == 'mean':
        threshold_price = price_series.mean()
    elif threshold == 'upper_quartile':
        threshold_price = price_series.quantile(0.75)
    else:
        threshold_price = price_series.median()
    
    df["Price_Binary"] = (price_series > threshold_price).map({True: "High", False: "Low-Medium"})
    print(f"\nBinary price category distribution (threshold={threshold}, ${threshold_price:.2f}):")
    print(df["Price_Binary"].value_counts())
    return df


def build_preprocessor(df: pd.DataFrame):
    binary_cols = []
    if "HasFrame" in df.columns:
        binary_cols.append("HasFrame")
    if "IsOriginal" in df.columns:
        binary_cols.append("IsOriginal")
    if "IsReal" in df.columns:
        binary_cols.append("IsReal")
    
    categorical_cols = df.select_dtypes(include="object").columns.tolist()
    categorical_cols = [col for col in categorical_cols if col not in {PRICE_COLUMN, PRICE_CATEGORY_COLUMN}]

    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    exclude_cols = {PRICE_COLUMN} | set(binary_cols)
    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]

    transformers = [
        ("categorical", OneHotEncoder(handle_unknown="ignore", sparse_output=False), categorical_cols),
        ("numeric", StandardScaler(), numeric_cols),
    ]
    
    if binary_cols:
        transformers.append(("binary", "passthrough", binary_cols))

    preprocessor = ColumnTransformer(
        transformers=transformers,
        remainder="drop",
    )

    all_features = categorical_cols + numeric_cols + binary_cols
    return preprocessor, all_features


def train_logistic_regression(X_train, y_train, X_test):
    preprocessor, feature_cols = build_preprocessor(X_train)

    log_pipeline = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            ("classifier", LogisticRegression(max_iter=1000, random_state=42, C=1.0)),
        ]
    )

    log_pipeline.fit(X_train, y_train)
    predictions = log_pipeline.predict(X_test)
    return predictions, log_pipeline


def train_random_forest(X_train, y_train, X_test, n_estimators=1000, max_depth=25):
    preprocessor, feature_cols = build_preprocessor(X_train)

    rf_pipeline = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            ("classifier", RandomForestClassifier(
                n_estimators=n_estimators,
                max_depth=max_depth,
                min_samples_split=2,
                min_samples_leaf=1,
                max_features='sqrt',
                bootstrap=True,
                random_state=42,
                n_jobs=-1,
                class_weight='balanced',
                oob_score=True
            )),
        ]
    )

    rf_pipeline.fit(X_train, y_train)
    predictions = rf_pipeline.predict(X_test)
    
    return predictions, rf_pipeline


def train_gradient_boosting(X_train, y_train, X_test, n_estimators=500):
    preprocessor, feature_cols = build_preprocessor(X_train)

    gb_pipeline = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            ("classifier", GradientBoostingClassifier(
                n_estimators=n_estimators,
                max_depth=12,
                learning_rate=0.03,
                min_samples_split=4,
                min_samples_leaf=2,
                subsample=0.8,
                max_features='sqrt',
                random_state=42
            )),
        ]
    )

    gb_pipeline.fit(X_train, y_train)
    predictions = gb_pipeline.predict(X_test)
    return predictions, gb_pipeline


def train_voting_ensemble(X_train, y_train, X_test):
    preprocessor, feature_cols = build_preprocessor(X_train)
    
    rf = RandomForestClassifier(
        n_estimators=300,
        max_depth=20,
        min_samples_split=2,
        random_state=42,
        n_jobs=-1,
        class_weight='balanced'
    )
    
    gb = GradientBoostingClassifier(
        n_estimators=200,
        max_depth=10,
        learning_rate=0.05,
        random_state=42
    )
    
    log = LogisticRegression(max_iter=1000, random_state=42, C=1.0)
    
    X_train_processed = preprocessor.fit_transform(X_train)
    X_test_processed = preprocessor.transform(X_test)
    
    voting_clf = VotingClassifier(
        estimators=[
            ('rf', rf),
            ('gb', gb),
            ('log', log)
        ],
        voting='soft',
        weights=[2, 2, 1]
    )
    
    voting_clf.fit(X_train_processed, y_train)
    predictions = voting_clf.predict(X_test_processed)
    
    return predictions, voting_clf


def train_xgboost(X_train, y_train, X_test):
    if not HAS_XGBOOST:
        return None, None
    
    preprocessor, feature_cols = build_preprocessor(X_train)

    xgb_pipeline = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            ("classifier", XGBClassifier(
                n_estimators=500,
                max_depth=8,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
                eval_metric='mlogloss'
            )),
        ]
    )

    xgb_pipeline.fit(X_train, y_train)
    predictions = xgb_pipeline.predict(X_test)
    return predictions, xgb_pipeline


def train_regression_approach(X_train, y_train, X_test, df_train, df_test):
    preprocessor, feature_cols = build_preprocessor(X_train)
    
    y_train_price = df_train[PRICE_COLUMN].values
    y_test_price = df_test[PRICE_COLUMN].values
    
    price_tertiles = pd.Series(y_train_price).quantile([0, 1/3, 2/3, 1]).values
    price_tertiles[0] = y_train_price.min() - 1
    price_tertiles[-1] = y_train_price.max() + 1
    
    rf_reg_pipeline = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            ("regressor", RandomForestRegressor(
                n_estimators=500,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )),
        ]
    )
    
    rf_reg_pipeline.fit(X_train, y_train_price)
    price_predictions = rf_reg_pipeline.predict(X_test)
    
    category_predictions = pd.cut(price_predictions, bins=price_tertiles, 
                                   labels=["Low", "Medium", "High"], include_lowest=True)
    
    mae = mean_absolute_error(y_test_price, price_predictions)
    rmse = np.sqrt(mean_squared_error(y_test_price, price_predictions))
    r2 = r2_score(y_test_price, price_predictions)
    
    print(f"  Regression MAE: ${mae:.2f}")
    print(f"  Regression RMSE: ${rmse:.2f}")
    print(f"  Regression R¬≤: {r2:.4f}")
    
    return category_predictions, rf_reg_pipeline


def analyze_features(df: pd.DataFrame, price_col: str):
    print("\n" + "=" * 50)
    print("FEATURE ANALYSIS")
    print("=" * 50)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols = [col for col in numeric_cols if col != price_col]
    
    if len(numeric_cols) > 0:
        correlations = df[numeric_cols + [price_col]].corr()[price_col].abs().sort_values(ascending=False)
        print("\nTop correlations with price:")
        for col, corr in correlations.items():
            if col != price_col and not pd.isna(corr):
                print(f"  {col:20s}: {corr:.4f}")


def main():
    df = ensure_clean_data()
    
    print("\n" + "=" * 50)
    print("FEATURE ENGINEERING")
    print("=" * 50)
    df = feature_engineering(df)
    print("‚úì Parsed Size column into Width, Height, Area, AspectRatio, Perimeter, LogArea")
    print("‚úì Added interaction features: Area_Delivery, Original_Real, Frame_Area, PricePerArea")
    
    analyze_features(df, PRICE_COLUMN)
    
    if "PricePerArea" in df.columns:
        df = df.drop(columns=["PricePerArea"])
    
    df = assign_price_categories(df)

    if df[PRICE_CATEGORY_COLUMN].isnull().any():
        raise ValueError("Price category contains NaN values. Check the binning logic.")

    feature_columns = [col for col in df.columns if col not in {PRICE_COLUMN, PRICE_CATEGORY_COLUMN}]
    X = df[feature_columns]
    y = df[PRICE_CATEGORY_COLUMN]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42,         stratify=y
    )

    train_indices = X_train.index
    test_indices = X_test.index

    print(f"\nTraining set size: {len(X_train)}, Test set size: {len(X_test)}")
    print(f"Number of features: {len(feature_columns)}")

    results = {}

    print("\n" + "=" * 50)
    print("LOGISTIC REGRESSION")
    print("=" * 50)
    log_preds, log_model = train_logistic_regression(X_train, y_train, X_test)
    log_acc = accuracy_score(y_test, log_preds)
    results['Logistic Regression'] = log_acc
    print(classification_report(y_test, log_preds, zero_division=0))
    print(f"Logistic Regression Accuracy: {log_acc:.4f}")

    print("\n" + "=" * 50)
    print("RANDOM FOREST CLASSIFIER (IMPROVED)")
    print("=" * 50)
    rf_preds, rf_model = train_random_forest(X_train, y_train, X_test, n_estimators=1500, max_depth=30)
    rf_acc = accuracy_score(y_test, rf_preds)
    results['Random Forest (1500 trees)'] = rf_acc
    print(classification_report(y_test, rf_preds, zero_division=0))
    print(f"Random Forest Accuracy: {rf_acc:.4f}")

    # Gradient Boosting Classifier
    print("\n" + "=" * 50)
    print("GRADIENT BOOSTING CLASSIFIER")
    print("=" * 50)
    gb_preds, gb_model = train_gradient_boosting(X_train, y_train, X_test, n_estimators=600)
    gb_acc = accuracy_score(y_test, gb_preds)
    results['Gradient Boosting (600 trees)'] = gb_acc
    print(classification_report(y_test, gb_preds, zero_division=0))
    print(f"Gradient Boosting Accuracy: {gb_acc:.4f}")

    print("\n" + "=" * 50)
    print("VOTING ENSEMBLE (RF + GB + Logistic)")
    print("=" * 50)
    ensemble_preds, ensemble_model = train_voting_ensemble(X_train, y_train, X_test)
    ensemble_acc = accuracy_score(y_test, ensemble_preds)
    results['Voting Ensemble'] = ensemble_acc
    print(classification_report(y_test, ensemble_preds, zero_division=0))
    print(f"Voting Ensemble Accuracy: {ensemble_acc:.4f}")

    if HAS_XGBOOST:
        print("\n" + "=" * 50)
        print("XGBOOST CLASSIFIER")
        print("=" * 50)
        xgb_preds, xgb_model = train_xgboost(X_train, y_train, X_test)
        if xgb_preds is not None:
            xgb_acc = accuracy_score(y_test, xgb_preds)
            results['XGBoost'] = xgb_acc
            print(classification_report(y_test, xgb_preds, zero_division=0))
            print(f"XGBoost Accuracy: {xgb_acc:.4f}")

    print("\n" + "=" * 50)
    print("REGRESSION APPROACH (Price ‚Üí Category)")
    print("=" * 50)
    df_train_reg = df.loc[train_indices]
    df_test_reg = df.loc[test_indices]
    
    reg_preds, reg_model = train_regression_approach(X_train, y_train, X_test, df_train_reg, df_test_reg)
    if reg_preds is not None:
        reg_acc = accuracy_score(y_test, reg_preds)
        results['Regression (Price‚ÜíCategory)'] = reg_acc
        print(classification_report(y_test, reg_preds, zero_division=0))
        print(f"Regression Approach Accuracy: {reg_acc:.4f}")

    print("\n" + "=" * 60)
    print("BINARY CLASSIFICATION (High vs Low-Medium)")
    print("=" * 60)
    
    for threshold_type in ['median', 'mean', 'upper_quartile']:
        df_binary = df.copy()
        df_binary = assign_binary_categories(df_binary, threshold=threshold_type)
        
        X_binary = df_binary[feature_columns]
        y_binary = df_binary["Price_Binary"]
        
        X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(
            X_binary, y_binary, test_size=0.2, random_state=42, stratify=y_binary
        )
        
        rf_bin_preds, _ = train_random_forest(X_train_bin, y_train_bin, X_test_bin, n_estimators=1500, max_depth=30)
        gb_bin_preds, _ = train_gradient_boosting(X_train_bin, y_train_bin, X_test_bin, n_estimators=700)
        
        rf_bin_acc = accuracy_score(y_test_bin, rf_bin_preds)
        gb_bin_acc = accuracy_score(y_test_bin, gb_bin_preds)
        
        print(f"\n--- Threshold: {threshold_type.upper()} ---")
        print(f"RF Binary Accuracy: {rf_bin_acc:.4f} ({rf_bin_acc*100:.2f}%)")
        print(f"GB Binary Accuracy: {gb_bin_acc:.4f} ({gb_bin_acc*100:.2f}%)")
        
        results[f'RF Binary ({threshold_type})'] = rf_bin_acc
        results[f'GB Binary ({threshold_type})'] = gb_bin_acc
        
        if threshold_type == 'upper_quartile':
            print("\nüìä Detailed Report (Upper Quartile Threshold - BEST PERFORMING):")
            print("Random Forest (Binary):")
            print(classification_report(y_test_bin, rf_bin_preds, zero_division=0))
            print("Gradient Boosting (Binary):")
            print(classification_report(y_test_bin, gb_bin_preds, zero_division=0))
        elif threshold_type == 'median':
            print("\nDetailed Report (Median Threshold):")
            print("Random Forest (Binary):")
            print(classification_report(y_test_bin, rf_bin_preds, zero_division=0))
            print("Gradient Boosting (Binary):")
            print(classification_report(y_test_bin, gb_bin_preds, zero_division=0))

    print("\n" + "=" * 60)
    print("MODEL COMPARISON SUMMARY")
    print("=" * 60)
    for model_name, accuracy in sorted(results.items(), key=lambda x: x[1], reverse=True):
        print(f"{model_name:35s}: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    best_model = max(results.items(), key=lambda x: x[1])
    improvement = ((best_model[1] - 0.3333) / 0.3333) * 100
    
    balanced_results = {k: v for k, v in results.items() if 'upper_quartile' not in k}
    best_balanced = max(balanced_results.items(), key=lambda x: x[1])
    balanced_improvement = ((best_balanced[1] - 0.3333) / 0.3333) * 100
    
    print(f"\nüèÜ Best Overall Model: {best_model[0]}")
    print(f"   Accuracy: {best_model[1]:.4f} ({best_model[1]*100:.2f}%)")
    print(f"   Improvement over baseline (33.33%): {improvement:+.2f}%")
    print(f"   ‚ö†Ô∏è  Note: Upper quartile threshold has class imbalance (75.5% vs 24.5%)")
    
    print(f"\n‚≠ê Best Balanced Model: {best_balanced[0]}")
    print(f"   Accuracy: {best_balanced[1]:.4f} ({best_balanced[1]*100:.2f}%)")
    print(f"   Improvement over baseline (33.33%): {balanced_improvement:+.2f}%")
    print(f"   ‚úì Better balanced classes, more practical for production use")
    
    print("\n" + "=" * 60)
    print("KEY IMPROVEMENTS IMPLEMENTED:")
    print("=" * 60)
    print("‚úì Feature engineering: Size parsing, Area, AspectRatio, Perimeter")
    print("‚úì Interaction features: Area√óDelivery, Original√óReal, Frame√óArea")
    print("‚úì Binary classification: Simplified to High vs Low-Medium")
    print("‚úì Multiple thresholds: Median, Mean, Upper Quartile")
    print("‚úì Enhanced models: 1500+ trees, deeper forests, tuned hyperparameters")
    print("‚úì Feature correlation analysis")
    print("=" * 60)


if __name__ == "__main__":
    main()
